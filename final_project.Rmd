---
title: "final_project"
author: "Student 1 & Student 2"
date: "2024-12-01"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
public_data <- read.csv('./data/NA_HQ_public_data.csv')
svi_data <- read.csv('./data/SVI2018_US.csv')
us_cities <- read.csv('./data/uscities.csv')
library(pander)
```

# 0. Contribution Statement

## Student 1

Student 1 mostly worked on questions 1, 3, and the advanced analysis. 

## Student 2

Student 2 mostly worked on questions 2, 4, and the intro/conclusion. 

# Artificial Intelligence
Artificial intelligence was used to provide a general guideline as to how we could answer the questions and to what depth. It was also used to help improve the phrasing of our analyses.

\pagenumbering{arabic}

# Introduction

Cities provide a glimpse into the local population of an area and are reflective of the culture and lifestyles in the region in which they are located. However, not all cities are created equal, as different socioeconomic factors within the population and the surrounding region impact the quality of life of residents.

### Data

Our primary dataset consists of a collection of various studies done by the CDC in 2018 that detail various characteristics of a particular city and its demographics, ranging from the percentage of people that are unemployed to the percentage of civilian noninstitutionalized population with a disability. There are 72836 entries.

We include a secondary dataset in our **Advanced Analysis** section of our report that consists of the climate action surveys of major international corporations and if they responded to the survey or not. There are two surveys collected (Water and Climate Change) between 2018 and 2020, inclusive.

### Objective

Our objective is to analyze and document the potential causal relationships between city demographic factors and economic outcomes in the dataset, with an emphasis on the influence of unemployment rate (`EP_UNEMP`) on per capita income, `EP_PCI`.

\pagebreak

# Basic Analysis

## **Question 1: Simple Regression** - Fit a regression line of the data predicting a city’s `EP_PCI` (estimated per capita income)* based on the estimated proportion of unemployment `EP_UNEMP`. How well does the regression line fit this relationship?

### Methods

**Data Cleaning & Preparation**

```{r echo=FALSE}
pander(quantile(svi_data$EP_UNEMP, p = c(0, 0.25, 0.5, 0.75, 1)) )
sum(svi_data$EP_UNEMP == -999)
# We seem to get some -999, which doesn't make sense in the context of estimated unemployment rates. In this column, we have 546 values / 72837 observations encoded as -999. 

pander(quantile(svi_data$EP_PCI, p = c(0, 0.25, 0.5, 0.75, 1)))
sum(svi_data$EP_PCI == -999)
# The EP_PCI column also has 481 values encoded as -999. 

# Change the -999 to NaNs
svi_data$EP_UNEMP[svi_data$EP_UNEMP == -999] <- NA
svi_data$EP_PCI[svi_data$EP_PCI == -999] <- NA

# Filter out the NaNs and store them in another dataset called "data".
data <- svi_data[complete.cases(svi_data$EP_PCI, svi_data$EP_UNEMP), ]
```
We checked for outliers and removed unusual values. In the summary statistics, we can see -999 appeared quite frequently. We replaced these values with NA and then filtered them out. This step is necessary for regression model preparation to avoid getting misleading interpretations since negative values do not make sense. 

We want to create a regression model for `EP_PCI`, the estimated per capita income, based on the estimated proportion of unemployment, `EP_UNEMP`. We are predicting `EP_PCI` because it describes the *rate* of unemployment and is normalized against population, unlike `E_PCI`, which describes the estimate *count* the unemployed.

We will check the conditions for fitting a simple linear regression: linearity, homoscedasticity, independence, and normality.

1.  **Linearity** - From our scatterplot, there appears to be a nonlinear negative trend between unemployment rate and estimated per capita income. As the estimated unemployment rate increases, the estimated per capita income exponentially decreases, which suggests a simple linear regression model is inappropriate. 

```{r echo=FALSE}
plot(data$EP_UNEMP, data$EP_PCI, 
     xlab = 'Estimated Unemployment Rate', 
     ylab = 'Estimated Per Capita Income', 
     main = 'Trend in Estimated Per Capita Income Based on Estimated Unemp. Rate')
```

2.  **Homoscedasticity**: The residuals do *not* have constant variance, which indicates heteroscedasticity.

3.  **Independence**: Because the residuals exhibit a clear pattern, independence may be violated.

```{r echo=FALSE}
model <- lm(EP_PCI ~ EP_UNEMP, data = data)
res <- residuals(model)
plot(data$EP_UNEMP, res, 
     xlab = 'Estimated Unemployment Rate', 
     ylab = 'Estimated Per Capita Income', 
     main = 'Trend in Estimated Per Capita Income Based on Estimated Unemp. Rate')
abline(0,0, col = 'red')
```
4.  **Normality**: A QQ plot of our residuals shows our data deviates from the y = x line. Its slight curve upward suggests the distribution of our residuals are right-skewed.

```{r echo = FALSE}
qqnorm(res)
qqline(res, col = 'red')
```

### Analysis
To address the issues of heteroscedasticity and linearity, we applied a log transformation to `EP_UNEMP` and `EP_PCI` because log transformations are often used to stabilize variance and help make data more suitable for regression. 
```{r echo = FALSE}
# double log scale
data$log_unemp <- log(data$EP_UNEMP + 1e-6)
data$log_pci <- log(data$EP_PCI + 1e-6)

log_model <- lm(log_pci ~ log_unemp, data = data)

# change predictions back to the original scale
predictions <- predict(log_model, newdata = data.frame(log_unemp = data$log_unemp))
predictions <- exp(predictions)

# Plot the scatterplot
plot(data$log_unemp, data$EP_PCI,
     main = 'Estimated Per Capita Income as a Function of Log(Estimated Populated Rate)',
     xlab = 'Log(Estimated Population Rate)',
     ylab = 'Estimated Per Capita Income')

lines(data$log_unemp, predictions, col='red')

# Model evaluation
# evaluating log model
res <- residuals(log_model)
mse <- mean(res ** 2) # 0.201

# evaluating original model for interpretation purposes
mse_original <- mean((data$EP_PCI - predictions)^2)
```

With our log-transformed model, we have an improved MSE of about 0.201, which represents the averages squared difference between the actual values of the estimated per capita income and the predicted values. This number is pretty low, which indicates our model performed well. This is significantly lower than the original MSE (no log transformation)  of 301,671,723. 

### Conclusion
Based on these findings, the log transformation significantly improved the model fit and addressed the issues of linearity, homoscedasticity, independence, and normality. Now that our model yields a lower MSE, our **log-transformed model is a more accurate predictor** of per capita income (EP_PCI) based on unemployment rate. 

Here is a recap of how the relationship between the estimated unemployment rate (EP_UNEMP) and the estimated per capita income (EP_PCI) do not meet the assumptions needed to fit a simple linear regression model without a transformation:

1. **Linearity:** The scatterplot shows a nonlinear, exponential relationship between EP_UNEMP and EP_PCI, which suggests a simple linear model is not the most effective.

2. **Homoscedasticity:** The residuals from the untransformed model show clear patterns and non-constant variance, which reaffirms the idea that our model does not perform well without a log transformation.

3. **Independence:** Similar to the point addressed in #2, the residual plots show clear patterns, which indicate a basic linear model is not suitable for this distribution. 

4. **Normality**: The QQ Plot demonstrates a slight right skew, which means that the residuals from a simple linear model are not normal, and so a transformation is necessary. 

## **Question 2: Distribution Analysis** - How do the distributions of `EP_PCI` and `EP_UNEMP` compare to one another and among other features?

### Methods

To see the distributions of the `EP_UNEMP` and `EP_PCI`, we graphed each feature as a histogram.

```{r, echo=FALSE}
par(mfrow=c(1, 2))
hist(data$EP_UNEMP, main = "EP_UNEMP", xlab = "Unemployment (%)", col = "lightblue")

hist(data$EP_PCI, main = "EP_PCI", xlab = "Per Capita Income", col = "lightgreen")
```

Additionally, we compared these distributions to other features that we believe would be related to this linear model, such as the percentage of people who are uninsured (`EP_UNINSUR`) and the percentage of mobile homes in the city (`EP_MOBILE`).

```{r, echo=FALSE}
# remove Nans from data
data[data == -999] <- NA
data <- data[complete.cases(data), ]

par(mfrow=c(1, 2))

# plot histograms of EP_UNINSUR and EP_MOBILE
hist(data$EP_UNINSUR, main = "EP_UNINSUR", xlab = "Uninsured (%)", col = "pink")
hist(data$EP_MOBILE, main = "EP_MOBILE", xlab = "Mobile Homes (%)", col = "yellow")
```

### Analysis

We will conduct a **KS Analysis** to test whether the distribution of these features in our dataset are statistically similar.

-   **H0:** The distribution of both features are the same.

-   **H1:** The distribution of the features differ.

We meet the conditions to perform this test because these features are independent from one another and the data is continuous.

```{r, echo=FALSE}
# KS test
ks_test <- ks.test(data$EP_UNEMP, data$EP_PCI, exact=TRUE)
pander(ks_test)

ks_test <- ks.test(data$EP_UNEMP, data$EP_UNINSUR, exact=TRUE)
pander(ks_test)

ks_test <- ks.test(data$EP_UNEMP, data$EP_MOBILE, exact=TRUE)
pander(ks_test)

ks_test <- ks.test(data$EP_PCI, data$EP_UNINSUR, exact=TRUE)
pander(ks_test)

ks_test <- ks.test(data$EP_PCI, data$EP_MOBILE, exact=TRUE)
pander(ks_test)
```

### Conclusion

When looking at our visualizations for the features, it appears that are is no significant difference between their distributions, as they are all right-skewed. However, our results from the KS test reflect the opposite conclusion, as the test statistics for each test are less than the test statistic at alpha = 0.05. As a result, we cannot conclude if the distributions of these features are statistically similar, and therefore we cannot conclude that they affect the value of `EP_UNEMP`. 

## **Question 3: Hypothesis Testing** - Do cities in states with higher percentages of `EP_PCI` (estimated per capita income) have significantly fewer climate action responses compared to those with lower percentages?

### Methods
H0: There is no significant difference between the number of climate action responses in states with higher percentages of EP_PCI compared to states with lower percentages. 
H1: States with higher percentages of EP_PCI have significantly _fewer_ climate action responses compared to states with lower percentages of EP_PCI. 

```{r echo = FALSE}
library(dplyr)
filtered <- public_data %>%
  filter(
    hq_country == "United States of America", # filter for only locations in the US
    address_state != "", 
    address_city != "",
    address_state != 'DO NOT SEND LETTER', # doesn't have state or city
    address_state != 'England' # London, England, mistakenly recorded as a city, state in the US
  )

# Some states are spelled out by name; change to state abbreviation
filtered <- filtered %>%
  mutate(address_state = recode(
    address_state, 
    "Massachusetts" = "MA", 
    "Pennsylvania" = "PA",
    'California' = 'CA',
    'Wisconsin' = 'WI',
    'Virginia' = 'VA',
    'Texas' = 'TX',
    'Ca' = 'CA',
    'Utah' = 'UT',
    'Arizona' = 'AZ',
    'Chicago' = 'IL',
    'Illinois' = 'IL',
    'District of Columbia' = 'DC',
    'Washington' = 'WA',
    'Rhode Island' = 'RI',
    'Colorado' = 'CO',
    'COLORADO' = 'CO',
    'Michigan' = 'MI',
    'New York' = 'NY',
    'Ohio' = 'OH',
    'Delaware' = 'DE',
    'Georgia' = 'GA',
    'Minnesota' = 'MN',
    'Missouri' = 'MO',
    'Maryland' = 'MD'
  ))

filtered_df <- as.data.frame(filtered)

# Merge the data filtered from the SVI DataFrame (shows the the socieoeconomic vulnerability index for various cities in the US) with the public city data. 
merged_df <- merge(
  x = filtered_df[, c('theme', 'address_state')],
  y = data[, c('ST_ABBR', 'EP_PCI')],
  by.x = 'address_state',
  by.y = 'ST_ABBR',
  all = FALSE
)
```
We have just created a DataFrame that shows the state, whether they took a climate or water action, and the EP_PCI (estimated per capita income) for each city recorded. 
NOTE: Cities are not included in our merged DataFrame since we are just interested in the cities' states. 

### Analysis
```{r echo = FALSE}
median_pci <- median(merged_df$EP_PCI, na.rm = TRUE)

# Create a column that says whether the city's EP_PCI is higher or lower compared to the median value
merged_df <- merged_df %>%
  mutate(pci_status = ifelse(EP_PCI > median_pci, 'Higher', 'Lower'))

# Our data currently contains climate change and water action data. We will filter for climate action data since it is what we care about more. 
climate_data <- merged_df %>%
  filter(theme == 'Climate Change')

# Create a table that summarizes the number of actions taken per each location that are related to climate change. 
summary_df <- climate_data %>%
  group_by(address_state, pci_status) %>%
  summarize(
    count = n(),
    .groups = 'drop'
  )
```
The summary_df keeps track of the number of climate change-related actions taken for various cities grouped by state and PCI status (either higher or lower than the median value). 

```{r echo = FALSE}
# Is the distribution of counts of climate action normally distributed for values higher than the median?
higher_counts <- summary_df$count[summary_df$pci_status == "Higher"]
shapiro.test(higher_counts)

lower_counts <- summary_df$count[summary_df$pci_status == "Lower"]
shapiro.test(lower_counts)
```
Neither of the higher nor lower count groups are normally distributed, so we need to use a Mann-Whitney U Test for comparing the mean counts of climate action for the groups with EP_PCI lower vs. higher than the median EP_PCI. 

```{r echo = FALSE}
wilcox.test(count ~ pci_status, data = summary_df, alternative = 'less')
```
Because (p = 0.3852) > (alpha = 0.05), we fail to reject the null hypothesis. Cities in states with higher percentages of EP_PCI _do not_ have significantly fewer climate action responses compared to those with lower percentages. 

### Conclusion
We merged the SVI Data with the city data and performed data cleaning to keep track of the number of cities with an EP_PCI status higher or lower than the median EP_PCI status. Since Shapiro-Wilk tests showed that our data for each group - Higher and Lower EP_PCI - were both not normally distributed, we had to use a Mann Whitney U Test (aka Wilcoxon Rank Sum Test) to compare the two groups since it is a non-parametric statistical test. Since we ended with a p-value of 0.3852 > 0.05, we fail to reject the null hypothesis. This means that cities in states with higher percentages of EP_PCI _do not_ have significantly fewer climate action responses compared to those with lower percentages. 

## **Question 4: Correlation Analysis** - Is there a significant correlation between the `EP_PCI` and `EP_UNEMP`? How strong is the relationship, and what does it suggest about the role of unemployment in estimated per capita income?

### Methods

To understand the correlation between these two features, we calculate the Pearson correlation for the features.

```{r, echo=FALSE}

correlation <- cor(data$EP_PCI, data$EP_UNEMP, use = "complete.obs", method = "pearson")
print(correlation)

```

### Analysis

Next, we test the significance of their correlation with the following hypotheses:

-   **H0:** The correlation between `EP_PCI` and `EP_UNEMP` is equal to zero.

-   **H1:** The correlation between `EP_PCI` and `EP_UNEMP` is not equal to zero.

```{r, echo=FALSE}
cor_test <- cor.test(data$EP_PCI, data$EP_UNEMP, method = "pearson")
pander(cor_test)
```

The correlation coefficient between `EP_PCI` and `EP_UNEMP` is **-0.4062332**, which signifies a moderate negative correlation between the two features. After performing a significance test on this coefficient, it is clear that it is statistically significant, as the test statistic is **-119.4** and the p-value is **0**. The negative sign in the test statistic shows that the correlation between these two features is negative, with its large magnitude reinforcing this relationship. Additionally, the p-value tells us to reject the null hypothesis, providing evidence that the correlation between the two feature is not equal to zero. 

### Conclusion

We can conclude that there is a statistically significant negative correlation between `EP_PCI` and `EP_UNEMP`, which is supported by the significance test we performed on the calculated correlation coefficient. This shows us that as unemployment increases within a city's population, the city's per capita income is expected to decrease. Additionally, the relationship between these two features is rather strong, as the magnitude of the correlation coefficient is a signiificant magnitude below zero.

## **Advanced Analysis: Multiple Regression** - Fit a regression line of the data predicting a city’s `EP_PCI` (estimated per capita income) based on the estimated proportion of unemployment `EP_UNEMP`, city population, and city density. How well does the regression line fit this relationship?

### Methods

We used XGBoost to optimize our prediction of EP_PCI, the estimated per capita income. 

### Data Preparation

To fit a regression line as specified above, we will need the following variables from the `data` dataset:

* Dependent Variable: EP_PCI - estimated per capita income

* Independent Variables: 

    * `E_NOHSDP` - percentage of people without a high school diploma (age 25+)
    * `EP_DISAB` - percentage of civilian non-institutionalized population who are disabled
    * `EP_MINRTY` - percentage of minority people
We choose these features because education is one of the strongest predictors of income, and we would expect people without a high school diploma to have fewer opportunities for job growth and thus, make a lower income than those with a higher degree. Disability status may also affect one's ability to work full time, leading them to make lower incomes. Cities with a higher percentage of disabled individuals may have a lower per capita income. Thirdly, minority populations may deal with socioeconomic disparities and restricted opportunities to education, wealth, and jobs. We also expect there to be a relationship between the proportion of minorities and income. Perhaps cities with a higher proportion of minorities may also have a lower per capita income. 

```{r echo = FALSE}
boxplot(data$EP_PCI, horizontal = TRUE, xlab = "Per Capita Income (EP_PCI)")
```
As you can see, there are many outliers in our data, so we removed data points that were below the 25th percentile or above the 75th percentile, per the IQR Rule. 

```{r echo = FALSE}
library(xgboost)
library(caret)

orig_features <- data[, c('EP_NOHSDP', 'EP_DISABL', 'EP_MINRTY')]
orig_target <- data$EP_PCI

Q1 <- quantile(orig_target, 0.25)
Q3 <- quantile(orig_target, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

data_filtered <- data[orig_target >= lower_bound & orig_target <= upper_bound, ]

# Update our features & target! 
features <- data_filtered[, c('EP_NOHSDP', 'EP_DISABL', 'EP_MINRTY')]
target <- data_filtered$EP_PCI
```
### Model Training
With the aid of the package Caret, we used 80% of the original dataset for training and the remaining 20% for testing. Our model used the objective function of regression with mean squared error and our evaluation metric was RMSE for better interpretability. 
``` {r echo = FALSE}
# Split into train + test
set.seed(123)
train_index <- createDataPartition(target, p = 0.8, list = FALSE)
train_features <- features[train_index, ]
train_target <- target[train_index]
test_features <- features[-train_index, ]
test_target <- target[-train_index]

# convert to matrices
train_matrix <- as.matrix(train_features)
test_matrix <- as.matrix(test_features)

# DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_target)
dtest <- xgb.DMatrix(data = test_matrix, label = test_target)

# hyperparameters
params <- list(
    objective = "reg:squarederror",
    eval_metric = "rmse",
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
)
```
The hyperparameters we used were learning rate = 0.1, max_depth = 6, subsample = 0.8 (80% of the samples used per boosting round to prevent overfitting), and colsample_bytree = 0.8 (80% of the features used per tree).

``` {r echo = FALSE}
# Train the model
xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 500,
    watchlist = list(train = dtrain, test = dtest),
    early_stopping_rounds = 10,
    verbose = 1
)

# PREDICTIONS
predictions <- predict(xgb_model, test_matrix)

# MODEL EVALUATIONS
# RMSE
rmse <- sqrt(mean((test_target - predictions) ^ 2))
cat("RMSE:", rmse, "\n")

# MAE
mae <- mean(abs(test_target - predictions))
cat("MAE:", mae, "\n")

# FEATURE IMPORTANCE
importance <- xgb.importance(feature_names = colnames(train_matrix), model = xgb_model)
xgb.plot.importance(importance)
```
The plot importance graph in the XG Boost package shows the importance of each of the features we used: earning a high school degree - 0.667, disability status - 0.167, minority group - 0.166. It seems as though the feature of having a high school degree `EP_NOHSDP` had the strongest impact on our model. 

### Analysis
```{r echo = FALSE}
cross_val <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 500,
  nfold = 5,
  metrics = 'rmse',
  early_stopping_rounds = 10,
  verbose = 1,
  prediction = TRUE
)
best_nrounds <- cross_val$best_iteration
cat("Best number of rounds:", best_nrounds, "\n") # 56
```
The best number of rounds we can use in our model is about 56. We will train our model again with the new optimal best number of rounds. 
```{r echo = FALSE}
# Train the model
new_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = best_nrounds,
    watchlist = list(train = dtrain, test = dtest),
    early_stopping_rounds = 10,
    verbose = 1
)

# PREDICTIONS
new_predictions <- predict(xgb_model, test_matrix)

# MODEL EVALUATIONS
# RMSE
new_rmse <- sqrt(mean((test_target - predictions) ^ 2))
cat("RMSE:", rmse, "\n")

# MAE
new_mae <- mean(abs(test_target - predictions))
cat("MAE:", mae, "\n")
```
After performing cross-validation, our model has an RMSE of about 7224.625 and an MAE of about 5243.471.
Our **normalized RMSE** would be 7224.625 / `mean(target)` = **0.2439**, which means that the model's predictions deviate from the actual values by about 24.39% on average. The **coefficient of variation (CV)** for this problem would be `sd(target) / mean(target)` = 0.3826. 1 - 0.2439/0.3826 = **0.3624**, which means our normalized RMSE is > 30% smaller than the coefficient of variation. This indicates that our model _does_ explain a significant amount of the variance in the data. 

### Conclusion
- At first glance, the model appears to have an extremely high RMSE, but when normalized against the mean(target), our model is actually reasonably accurate in predicting per capita income. 
- From the feature importance, we saw that `EP_NOHSDP` had the highest gain, which means it has the greatest contribution to reducing prediction errors. Contextually, this makes sense because low-income people may not be able to afford tuition at a university and with a lower degree, have a smaller skillset and thus, be paid less. 
- Since `EP_NOHSDP` was the most significant predictor, tutoring programs that help students in grades K-12 grasp a better understanding of their coursework may improve per capita income in the populaton since more students would be able to achieve high school diplomas. 

```{r echo = FALSE}
res <- test_target - predictions
plot(res, main = "Residuals of the Model", xlab = "Per Capita Income", ylab = "Residuals")
abline(h = 0, col = "red")
```
- Our model does a fairly good job at predicting! The residuals are roughly randomly scattered around 0. The model does appear to have some fanning present as income increases after $8,000, but is roughly still homoscedastic. 

## Conclusion & Discussion

In conclusion, our analysis of the causal relationship between per capita income (`EP_PCI`) and unemployment rate (`EP_UNEMP`) in the socioeconomic vulnerability index dataset reveal that there is a strong negative correlation between them, though a linear regression may not be the best model to capture the nuances of their relationship. Additionally, the distributions of these features appear to be similar, but there is no statistical evidence to support this. In our advanced analysis, we fit a multiple linear regression with the addition of new features that describe the population distribution and density of a city. Through further testing, we also discovered that the estimated percentage of no high school diploma among residents of a city had a significant correlation with the per capita income, and once included in our multiple linear regression, was able to explain a lot of the error we were experiencing in our previous models. 

One limitation that we noticed with our data was missing data in the socioeconomic vulnerability index dataset. Although we removed missing data, the resulting data could underestimate or overestimate certain features of the dataset, which inhibits our ability to make accurate calculations. Another limitation that we noticed was that many of the features in the dataset were highly related to one another, raising possible issues of multicollinearity in our predictive models.
